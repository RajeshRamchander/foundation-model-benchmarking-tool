experiment_name,payload_file,instance_type,concurrency,error_rate,prompt_token_count_mean,prompt_token_throughput,completion_token_count_mean,completion_token_throughput,latency_mean,transactions_per_minute
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_2000-3000.jsonl,ml.p4d.24xlarge,1,0.0,2503,1597,89,31,2.68,37
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_2000-3000.jsonl,ml.p4d.24xlarge,2,0.0,2503,1489,70,41,2.5,35
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_2000-3000.jsonl,ml.p4d.24xlarge,4,0.0,2503,2388,77,73,3.44,57
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_2000-3000.jsonl,ml.p4d.24xlarge,6,0.0,2515,3123,74,92,3.96,74
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_2000-3000.jsonl,ml.p4d.24xlarge,8,0.0,2503,3168,73,89,3.9,75
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_2000-3000.jsonl,ml.g5.48xlarge,1,0.0,2503,217,73,5,12.3,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_2000-3000.jsonl,ml.g5.48xlarge,2,0.0,2503,234,78,7,20.33,5
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_2000-3000.jsonl,ml.g5.48xlarge,4,0.0,2503,258,78,8,33.32,5
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_2000-3000.jsonl,ml.g5.48xlarge,6,0.0,2515,270,69,7,45.79,6
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_2000-3000.jsonl,ml.g5.48xlarge,8,0.41,4347,281,43,2,36.14,3
