aws:
  region: us-east-1
datasets:
- language: en
  max_length_in_tokens: 500
  min_length_in_tokens: 1
  payload_file: payload_{lang}_{min}-{max}.jsonl
- language: en
  max_length_in_tokens: 1000
  min_length_in_tokens: 500
  payload_file: payload_{lang}_{min}-{max}.jsonl
- language: en
  max_length_in_tokens: 2000
  min_length_in_tokens: 1000
  payload_file: payload_{lang}_{min}-{max}.jsonl
- language: en
  max_length_in_tokens: 3000
  min_length_in_tokens: 2000
  payload_file: payload_{lang}_{min}-{max}.jsonl
- language: en
  max_length_in_tokens: 4000
  min_length_in_tokens: 3000
  payload_file: payload_{lang}_{min}-{max}.jsonl
- language: en
  max_length_in_tokens: 3997
  min_length_in_tokens: 305
  payload_file: payload_{lang}_{min}-{max}.jsonl
experiments:
- accept_eula: true
  concurrency_levels:
  - 1
  - 2
  - 4
  - 6
  - 8
  deploy: true
  deployment_script: jumpstart.py
  env:
    ENDPOINT_SERVER_TIMEOUT: '3600'
    HF_MODEL_ID: /opt/ml/model
    MAX_INPUT_LENGTH: '4095'
    MAX_TOTAL_TOKENS: '4096'
    MODEL_CACHE_ROOT: /opt/ml/model
    SAGEMAKER_ENV: '1'
    SAGEMAKER_MODEL_SERVER_WORKERS: '1'
    SAGEMAKER_PROGRAM: inference.py
    SM_NUM_GPUS: '8'
  ep_name: llama-2-70b-g5-48xlarge
  image_uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04
  instance_count: 1
  instance_type: ml.g5.48xlarge
  model_id: meta-textgeneration-llama-2-70b
  model_name: llama2-70b
  model_version: '*'
  name: llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0
  payload_files:
  - payload_en_1-500.jsonl
  - payload_en_500-1000.jsonl
  - payload_en_1000-2000.jsonl
  - payload_en_2000-3000.jsonl
  - payload_en_3000-4000.jsonl
- accept_eula: true
  concurrency_levels:
  - 1
  - 2
  - 4
  - 6
  - 8
  deploy: true
  deployment_script: p4d_hf_tgi.py
  env:
    HEALTH_CHECK_TIMEOUT: 300
    INSTANCE_COUNT: 1
    MODEL_LOADING_TIMEOUT: '3600'
    NUMBER_OF_GPU: 8
  ep_name: llama-2-13b-p4d-24xlarge
  image_uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04
  instance_count: 1
  instance_type: ml.p4d.24xlarge
  model_id: meta-llama/Llama-2-70b-chat-hf
  model_name: meta-llama-Llama-2-70b-chat-hf
  model_version: '*'
  name: llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118
  payload_files:
  - payload_en_1-500.jsonl
  - payload_en_500-1000.jsonl
  - payload_en_1000-2000.jsonl
  - payload_en_2000-3000.jsonl
  - payload_en_3000-4000.jsonl
general:
  model_name: Llama2-70b
  name: llama2-70b-g5-p4d-tgi-v1
inference_parameters:
  do_sample: true
  max_new_tokens: 100
  temperature: 0.1
  top_k: 120
  top_p: 0.92
  truncate: at-prompt-token-length
metrics:
  dataset_of_interest: en_3000-4000
  weights:
    latenct_wt: 0.35
    price_per_tx_wt: 0.65
pricing:
  ml.g5.12xlarge: 7.09
  ml.g5.24xlarge: 10.18
  ml.g5.48xlarge: 20.36
  ml.inf2.24xlarge: 7.79
  ml.inf2.48xlarge: 15.58
  ml.p4d.24xlarge: 37.688
prompt:
  all_prompts_file: all_prompts.csv
  template_file: prompt_template.txt
results:
  all_metrics_file: all_metrics.csv
  per_inference_request_file: per_inference_request_results.csv
