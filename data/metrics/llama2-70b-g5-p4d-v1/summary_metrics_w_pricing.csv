experiment_name,payload_file,instance_type,concurrency,error_rate,prompt_token_count_mean,prompt_token_throughput,completion_token_count_mean,completion_token_throughput,latency_mean,transactions_per_minute
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,1,0.0,3478,237,70,4,15.54,3
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,2,0.0,3474,248,78,5,26.79,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,4,0.0,3468,276,67,5,42.51,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,6,0.77,14948,344,9,0,39.74,1
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,8,0.61,9489,306,33,0,38.27,1
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,1,0.0,3478,1449,3504,1457,2.7,24
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,2,0.0,3474,1605,3502,1616,4.11,27
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,4,0.0,3468,1576,3497,1587,6.68,26
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,6,0.0,3454,1583,3487,1596,9.18,27
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,8,0.0,3455,1624,3485,1637,9.91,27
