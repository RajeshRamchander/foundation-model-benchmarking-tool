experiment_name,payload_file,instance_type,concurrency,error_rate,prompt_token_count_mean,prompt_token_throughput,completion_token_count_mean,completion_token_throughput,latency_mean,transactions_per_minute
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,1,0.09,3431,4305,94,4,16.35,3
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,2,0.09,4105,300,89,5,23.59,3
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,4,0.08,3950,281,113,8,40.93,3
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,6,0.12,4013,252,64,3,63.1,3
llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,8,0.12,4055,193,52,2,93.73,2
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,1,0.09,3431,3366,105,4,17.24,3
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,2,0.09,4105,280,108,6,25.36,3
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,4,0.08,3950,295,102,7,40.31,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,6,0.08,3873,244,67,4,63.05,3
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,8,0.17,4261,229,42,2,77.17,3
