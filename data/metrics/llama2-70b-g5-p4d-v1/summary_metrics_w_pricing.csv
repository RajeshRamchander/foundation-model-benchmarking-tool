experiment_name,payload_file,instance_type,concurrency,error_rate,prompt_token_count_mean,prompt_token_throughput,completion_token_count_mean,completion_token_throughput,latency_mean,transactions_per_minute
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,1,0.0,3478,248,69,4,14.92,3
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,2,0.0,3474,255,79,5,25.73,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,4,0.0,3468,283,78,6,42.9,4
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,6,0.72,13990,344,12,0,36.31,1
llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0,payload_en_3000-4000.jsonl,ml.g5.48xlarge,8,0.66,10981,313,54,1,34.24,1
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,1,0.0,3478,1439,3504,1447,2.68,24
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,2,0.0,3474,1711,3501,1721,3.94,29
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,4,0.0,3468,1632,3495,1643,6.54,27
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,6,0.0,3454,1594,3484,1607,8.87,27
llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118,payload_en_3000-4000.jsonl,ml.p4d.24xlarge,8,0.0,3455,1619,3485,1632,9.93,27
