{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Model Deployments (for neuron in inf. based instances) with different model configurations\n",
    "\n",
    "-- Run the config.yaml file to store the models as well as your account execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import pathlib\n",
    "import importlib.util\n",
    "from globals import *\n",
    "from pathlib import Path\n",
    "from utils import load_config\n",
    "from typing import Dict, List, Optional\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.yml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33mllama2_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-18 15:34:20,035] p565 {2442081166.py:4} INFO - aws_region=us-east-1, sagemaker_execution_role=arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-18 15:34:20,036] p565 {2442081166.py:5} INFO - config={\n",
      "  \"general\": {\n",
      "    \"name\": \"llama2-inf2-g5-p4d-v1\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::015469603702:role/SageMakerRepoRole\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-12xlarge\",\n",
      "      \"instance_type\": \"ml.g5.12xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"4\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-24xlarge\",\n",
      "      \"instance_type\": \"ml.g5.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"4\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-48xlarge\",\n",
      "      \"instance_type\": \"ml.g5.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"8\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "aws_region = config['aws']['region']\n",
    "sagemaker_execution_role = config['aws']['sagemaker_execution_role']\n",
    "logger.info(f\"aws_region={aws_region}, sagemaker_execution_role={sagemaker_execution_role}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to deploy a model\n",
    "def deploy_model(experiment_config: Dict, aws_region: str, role_arn: str) -> Optional[Dict]:\n",
    "    logger.info(f\"going to deploy {experiment_config}, in {aws_region} with {role_arn}\")\n",
    "    model_deployment_result = None\n",
    "    deploy = experiment_config.get('deploy', False)\n",
    "    if deploy is False:\n",
    "        logger.error(f\"skipping deployment of {experiment_config['model_id']} because deploy={deploy}\")\n",
    "        return model_deployment_result\n",
    "    \n",
    "    try:        \n",
    "        module_name = Path(experiment_config['deployment_script']).stem\n",
    "        file_path = os.path.join(pathlib.Path().absolute().resolve(), SCRIPTS_DIR, f\"{module_name}.py\")\n",
    "        logger.info(f\"going to deploy using code in {file_path}\")\n",
    "\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        model_deployment_result = module.deploy(experiment_config, role_arn)\n",
    "        return model_deployment_result\n",
    "    \n",
    "    except ClientError as error:\n",
    "        print(f\"an error occurred: {error}\")\n",
    "        return model_deployment_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_deploy_model(experiment_config: Dict, role_arn: str, aws_region: str) -> str:\n",
    "    return await asyncio.to_thread(deploy_model, experiment_config, role_arn, aws_region)\n",
    "\n",
    "async def async_deploy_all_models(config: Dict) -> List[Dict]:\n",
    "    experiments: List[Dict] = config['experiments']\n",
    "    n: int = 4 # max concurrency so as to not get a throttling exception\n",
    "    experiments_splitted = [experiments[i * n:(i + 1) * n] for i in range((len(experiments) + n - 1) // n )]\n",
    "    results = []\n",
    "    for exp_list in experiments_splitted:\n",
    "        result = await asyncio.gather(*[async_deploy_model(m,\n",
    "                                                           config['aws']['region'],\n",
    "                                                           config['aws']['sagemaker_execution_role']) for m in exp_list])\n",
    "        results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-18 15:34:20,250] p565 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-12xlarge', 'instance_type': 'ml.g5.12xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '4', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-18 15:34:20,251] p565 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-24xlarge', 'instance_type': 'ml.g5.24xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '4', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-18 15:34:20,254] p565 {2014866166.py:13} INFO - going to deploy using code in /root/jumpstart-models-benchmarking-test-harness/scripts/jumpstart.py\n",
      "[2024-01-18 15:34:20,253] p565 {2014866166.py:13} INFO - going to deploy using code in /root/jumpstart-models-benchmarking-test-harness/scripts/jumpstart.py\n",
      "[2024-01-18 15:34:20,251] p565 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-48xlarge', 'instance_type': 'ml.g5.48xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '8', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-18 15:34:20,265] p565 {2014866166.py:13} INFO - going to deploy using code in /root/jumpstart-models-benchmarking-test-harness/scripts/jumpstart.py\n",
      "[2024-01-18 15:34:20,566] p565 {utils.py:160} INFO - NumExpr defaulting to 2 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-18 15:34:22,377] p565 {utils.py:475} INFO - Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-18 15:34:22,385] p565 {cache.py:437} WARNING - Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-18 15:34:22,408] p565 {utils.py:475} INFO - Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-18 15:34:22,411] p565 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-409\n",
      "[2024-01-18 15:34:22,412] p565 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-406\n",
      "[2024-01-18 15:34:22,414] p565 {cache.py:437} WARNING - Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-18 15:34:22,427] p565 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-424\n",
      "[2024-01-18 15:34:23,195] p565 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-12xlarge-1705592062\n",
      "[2024-01-18 15:34:23,240] p565 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-24xlarge-1705592062\n",
      "[2024-01-18 15:34:23,550] p565 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-48xlarge-1705592062\n",
      "[2024-01-18 15:34:23,591] p565 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-12xlarge-1705592062\n",
      "[2024-01-18 15:34:23,687] p565 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-24xlarge-1705592062\n",
      "[2024-01-18 15:34:23,989] p565 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-48xlarge-1705592062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------!!-!endpoint_names -> [{'endpoint_name': 'llama-2-13b-g5-12xlarge-1705592062', 'experiment_name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}, {'endpoint_name': 'llama-2-13b-g5-24xlarge-1705592062', 'experiment_name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}, {'endpoint_name': 'llama-2-13b-g5-48xlarge-1705592062', 'experiment_name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}], deployed in 339.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# async version\n",
    "s = time.perf_counter()\n",
    "endpoint_names = await async_deploy_all_models(config)\n",
    "elapsed_async = time.perf_counter() - s\n",
    "print(f\"endpoint_names -> {endpoint_names}, deployed in {elapsed_async:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-12xlarge-1705592062',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-12xlarge-1705592062',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-12xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 18, 15, 34, 25, 30000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 982000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 18, 15, 39, 6, 85000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '360b2949-75f3-4e2f-98d8-47254293f725',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '360b2949-75f3-4e2f-98d8-47254293f725',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '814',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-12xlarge-1705592062',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-12xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-409',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.12xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 564000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': 'f56b164f-380d-4e7f-b126-a9e59c046f90',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'f56b164f-380d-4e7f-b126-a9e59c046f90',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-409',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '4'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 67000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-409',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': 'b3063bfd-71d3-48e7-bd9e-7915ebbcee9f',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'b3063bfd-71d3-48e7-bd9e-7915ebbcee9f',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1138',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-24xlarge-1705592062',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-24xlarge-1705592062',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-24xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 18, 15, 34, 25, 159000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 24, 51000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 18, 15, 39, 10, 645000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '82af69fa-7bc4-4eef-8a8d-21ec473b7cc9',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '82af69fa-7bc4-4eef-8a8d-21ec473b7cc9',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '815',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-24xlarge-1705592062',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-24xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-406',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.24xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 653000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': '64ca3f32-faea-41bb-b31c-42927b1c1cb5',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '64ca3f32-faea-41bb-b31c-42927b1c1cb5',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-406',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '4'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 105000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-406',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': '2c040e95-3a8d-4256-8a3b-0fb79e5b6ee4',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '2c040e95-3a8d-4256-8a3b-0fb79e5b6ee4',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1138',\n",
       "     'date': 'Thu, 18 Jan 2024 15:39:59 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-48xlarge-1705592062',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-48xlarge-1705592062',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-48xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 18, 15, 34, 29, 533000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 28, 477000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 18, 15, 39, 51, 116000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '1c5b1fda-25b5-4307-b9b6-7ad23930fb7d',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '1c5b1fda-25b5-4307-b9b6-7ad23930fb7d',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '815',\n",
       "     'date': 'Thu, 18 Jan 2024 15:40:00 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-48xlarge-1705592062',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-48xlarge-1705592062',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-424',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.48xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 967000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': 'c7c24caa-1516-4d9e-8ae5-3c99f4056e0f',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'c7c24caa-1516-4d9e-8ae5-3c99f4056e0f',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 15:40:00 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-424',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '8'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 18, 15, 34, 23, 366000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-15-34-22-424',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': '9694d0ba-3d80-4721-af20-1065a9272fe7',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '9694d0ba-3d80-4721-af20-1065a9272fe7',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1138',\n",
       "     'date': 'Thu, 18 Jan 2024 15:40:00 GMT'},\n",
       "    'RetryAttempts': 0}}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_info_for_endpoint(ep: Dict) -> Dict:\n",
    "    ep_name = ep['endpoint_name']\n",
    "    experiment_name = ep['experiment_name']\n",
    "    if ep_name is None:\n",
    "        return None\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    endpoint = sm_client.describe_endpoint(EndpointName=ep_name)\n",
    "    endpoint_config = sm_client.describe_endpoint_config(EndpointConfigName=endpoint['EndpointConfigName'])\n",
    "    model_config = sm_client.describe_model(ModelName=endpoint_config['ProductionVariants'][0]['ModelName'])\n",
    "    info = dict(experiment_name=experiment_name,\n",
    "                endpoint=endpoint,\n",
    "                endpoint_config=endpoint_config,\n",
    "                model_config=model_config)\n",
    "    return info\n",
    "\n",
    "all_info = list(map(get_all_info_for_endpoint, [ep for ep in endpoint_names if ep is not None]))\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write all end point info to a file so that other notebooks can read it\n",
    "Path(ENDPOINT_LIST_FPATH).write_text(json.dumps(all_info, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
