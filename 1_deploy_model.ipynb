{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Model Deployments (for neuron in inf. based instances) with different model configurations\n",
    "\n",
    "-- Run the config.yaml file to store the models as well as your account execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/madhurpt/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import sagemaker\n",
    "from globals import *\n",
    "from typing import Dict\n",
    "from utils import load_config\n",
    "from pathlib import Path\n",
    "from sagemaker.predictor import Predictor\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.yml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33mllama2_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-05 08:52:10,336] p54752 {2442081166.py:4} INFO - aws_region=us-east-1, sagemaker_execution_role=arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-05 08:52:10,337] p54752 {2442081166.py:5} INFO - config={\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::015469603702:role/SageMakerRepoRole\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.7,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-conc=1-bs=4-tpd=12\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-24xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"payload_file\": \"payload_en_500-1000.jsonl\",\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"12\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-conc=1-bs=4-tpd=24\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-48xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": 1,\n",
      "      \"payload_file\": \"payload_en_500-1000.jsonl\",\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"results_per_inference_request_{datetime}.csv\",\n",
      "    \"all_experiments_file\": \"results_all_experiments_{datetime}.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "aws_region = config['aws']['region']\n",
    "sagemaker_execution_role = config['aws']['sagemaker_execution_role']\n",
    "logger.info(f\"aws_region={aws_region}, sagemaker_execution_role={sagemaker_execution_role}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to deploy a model\n",
    "def deploy_model(experiment_config: Dict, aws_region: str, role_arn: str) -> Dict:\n",
    "    try:\n",
    "        logger.info(f\"going to deploy {experiment_config}, in {aws_region} with {role_arn}\")\n",
    "        deploy = experiment_config.get('deploy', False)\n",
    "        if deploy is False:\n",
    "            logger.error(f\"skipping deployment of {experiment_config['model_id']} because deploy={deploy}\")\n",
    "            return None\n",
    "        model = JumpStartModel(\n",
    "            model_id=experiment_config['model_id'],\n",
    "            model_version=experiment_config['model_version'],\n",
    "            image_uri=experiment_config['image_uri'],\n",
    "            env=experiment_config['env'],\n",
    "            role=role_arn,\n",
    "            instance_type=experiment_config['instance_type']\n",
    "        )\n",
    "\n",
    "        # Deploy the model using asyncio.to_thread to run in a separate thread\n",
    "        ep_name = f\"{experiment_config['ep_name']}-{int(time.time())}\"\n",
    "        accept_eula = experiment_config.get('accept_eula')\n",
    "        if accept_eula is not None:\n",
    "            predictor = model.deploy(initial_instance_count=experiment_config['instance_count'],\n",
    "                                    accept_eula=accept_eula,\n",
    "                                    endpoint_name=ep_name)\n",
    "        else:\n",
    "            predictor = model.deploy(initial_instance_count=experiment_config['instance_count'],            \n",
    "                                     endpoint_name=ep_name)\n",
    "\n",
    "        return dict(endpoint_name=predictor.endpoint_name, experiment_name=experiment_config['name'])\n",
    "    except ClientError as error:\n",
    "        print(f\"an error occurred: {error}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_deploy_model(experiment_config: Dict, role_arn: str, aws_region: str) -> str:\n",
    "    return await asyncio.to_thread(deploy_model, experiment_config, role_arn, aws_region)\n",
    "\n",
    "async def async_deploy_all_models(config: Dict):\n",
    "    return await asyncio.gather(*[async_deploy_model(m,\n",
    "                                                     config['aws']['region'],\n",
    "                                                     config['aws']['sagemaker_execution_role']) for m in config['experiments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async version\n",
    "s = time.perf_counter()\n",
    "endpoint_names = await async_deploy_all_models(config)\n",
    "elapsed_async = time.perf_counter() - s\n",
    "print(f\"endpoint_names -> {endpoint_names}, deployed in {elapsed_async:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_info_for_endpoint(ep: Dict) -> Dict:\n",
    "    ep_name = ep['endpoint_name']\n",
    "    experiment_name = ep['experiment_name']\n",
    "    if ep_name is None:\n",
    "        return None\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    endpoint = sm_client.describe_endpoint(EndpointName=ep_name)\n",
    "    endpoint_config = sm_client.describe_endpoint_config(EndpointConfigName=endpoint['EndpointConfigName'])\n",
    "    model_config = sm_client.describe_model(ModelName=endpoint_config['ProductionVariants'][0]['ModelName'])\n",
    "    info = dict(experiment_name=experiment_name,\n",
    "                endpoint=endpoint,\n",
    "                endpoint_config=endpoint_config,\n",
    "                model_config=model_config)\n",
    "    return info\n",
    "\n",
    "all_info = list(map(get_all_info_for_endpoint, [ep for ep in endpoint_names if ep is not None]))\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4806"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write all end point info to a file so that other notebooks can read it\n",
    "Path(ENDPOINT_LIST_FPATH).write_text(json.dumps(all_info, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
