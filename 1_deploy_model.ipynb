{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Model Deployments (for neuron in inf. based instances) with different model configurations\n",
    "\n",
    "-- Run the config.yaml file to store the models as well as your account execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import pathlib\n",
    "import importlib.util\n",
    "from globals import *\n",
    "from pathlib import Path\n",
    "from utils import load_config\n",
    "from typing import Dict, List, Optional\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import yaml\n",
      "from enum import Enum\n",
      "from pathlib import Path\n",
      "\n",
      "CONFIG_FILE: str = \"config.yml\"\n",
      "with open(CONFIG_FILE, 'r') as file:\n",
      "    config = yaml.safe_load(file)\n",
      "\n",
      "DATA_DIR: str = \"data\"\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \"prompts\")\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \"metrics\", config['general']['name'])\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \"per_inference\")\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \"per_chunk\")\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \"models\", config['general']['name'])\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \"dataset\")\n",
      "SCRIPTS_DIR: str = \"scripts\"\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\n",
      "TOKENIZER_DIR = 'llama2_tokenizer'\n",
      "\n",
      "_ = list(map(lambda x: os.makedirs(x, exist_ok=True), DIR_LIST))\n",
      "\n",
      "ENDPOINT_LIST_FPATH:str = os.path.join(MODELS_DIR, \"endpoints.json\")\n",
      "REQUEST_PAYLOAD_FPATH:str = os.path.join(PROMPTS_DIR, \"payload.jsonl\")\n",
      "RESULTS_FPATH:str = os.path.join(METRICS_DIR, \"results.csv\")\n",
      "class TRUNCATE_POLICY(str, Enum):\n",
      "    AT_PROMPT_TOKEN_LENGTH = 'at-prompt-token-length'\n",
      "\n",
      "# misc. metrics related\n",
      "PLACE_HOLDER: int = -1705338041\n",
      "\n",
      "# metric filenames\n",
      "COUNTS_FNAME: str = \"experiment_counts.csv\"\n",
      "ERROR_RATES_FNAME: str = \"error_rates.csv\"\n",
      "RESULTS_DESC_MD_FNAME: str = \"results.md\"\n",
      "\n",
      "# plot filenames\n",
      "ERROR_RATES_PLOT_TEXT: str = \"Error rates for different concurrency levels and instance types\"\n",
      "ERROR_RATES_PLOT_FNAME: str = \"error_rates.png\"\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: str = \"Tokens vs latency for different concurrency levels and instance types\"\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: str = \"tokens_vs_latency.png\"\n",
      "\n",
      "\n",
      "LATENCY_BUDGET: int = 20\n",
      "\n",
      "OVERALL_RESULTS_MD: str = \"\"\"\n",
      "# Results for performance benchmarking\n",
      "\n",
      "**Last modified (UTC): {dttm}**\n",
      "\n",
      "## Summary\n",
      "\n",
      "The following table provides the best combinations for running inference for different sizes prompts on different instance types.\n",
      "|Dataset   | Instance type   | Recommendation   |\n",
      "|---|---|---|\n",
      "\"\"\"\n",
      "\n",
      "## Dataset=`{dataset}`, instance_type=`{instance_type}`\n",
      "RESULT_DESC: str = \"\"\"The best option for staying within a latency budget of `{latency_budget} seconds` on a `{instance_type}` for the `{dataset}` dataset is a `concurrency level of {concurrency}`. A concurrency level of {concurrency} achieves an `average latency of {latency_mean} seconds`, for an `average prompt size of {prompt_size} tokens` and `completion size of {completion_size} tokens` with `{tpm} transactions/minute`.\"\"\"\n",
      "\n",
      "RESULT_ROW: str = \"|`{dataset}`|`{instance_type}`|{desc}|\"\n",
      "\n",
      "RESULT_FAILURE_DESC: str = \"\"\"This experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `{latency_budget} seconds` on a `{instance_type}` for the `{dataset}` dataset.\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-17 22:37:54,060] p51768 {2442081166.py:4} INFO - aws_region=us-east-1, sagemaker_execution_role=arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:37:54,062] p51768 {2442081166.py:5} INFO - config={\n",
      "  \"general\": {\n",
      "    \"name\": \"llama2-inf2-g5-p4d-v1\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::015469603702:role/SageMakerRepoRole\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"llama2-70b-chat-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118\",\n",
      "      \"model_id\": \"llama2-70b-chat\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-70b-MODEL_chat\",\n",
      "      \"ep_name\": \"llama-2-70b-chat-p4d-24xlarge\",\n",
      "      \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"p4d_hf_tgi.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"MODEL_LOADING_TIMEOUT\": \"3600\",\n",
      "        \"NUMBER_OF_GPU\": 8,\n",
      "        \"INSTANCE_COUNT\": 1,\n",
      "        \"HEALTH_CHECK_TIMEOUT\": 300\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-12xlarge\",\n",
      "      \"instance_type\": \"ml.g5.12xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"4\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-24xlarge\",\n",
      "      \"instance_type\": \"ml.g5.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"4\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-13b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-g5-48xlarge\",\n",
      "      \"instance_type\": \"ml.g5.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"8\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=12\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-24xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"12\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-48xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "aws_region = config['aws']['region']\n",
    "sagemaker_execution_role = config['aws']['sagemaker_execution_role']\n",
    "logger.info(f\"aws_region={aws_region}, sagemaker_execution_role={sagemaker_execution_role}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to deploy a model\n",
    "def deploy_model(experiment_config: Dict, aws_region: str, role_arn: str) -> Optional[Dict]:\n",
    "    logger.info(f\"going to deploy {experiment_config}, in {aws_region} with {role_arn}\")\n",
    "    model_deployment_result = None\n",
    "    deploy = experiment_config.get('deploy', False)\n",
    "    if deploy is False:\n",
    "        logger.error(f\"skipping deployment of {experiment_config['model_id']} because deploy={deploy}\")\n",
    "        return model_deployment_result\n",
    "    \n",
    "    try:        \n",
    "        module_name = Path(experiment_config['deployment_script']).stem\n",
    "        file_path = os.path.join(pathlib.Path().absolute().resolve(), SCRIPTS_DIR, f\"{module_name}.py\")\n",
    "        logger.info(f\"going to deploy using code in {file_path}\")\n",
    "\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        model_deployment_result = module.deploy(experiment_config, role_arn)\n",
    "        return model_deployment_result\n",
    "    \n",
    "    except ClientError as error:\n",
    "        print(f\"an error occurred: {error}\")\n",
    "        return model_deployment_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_deploy_model(experiment_config: Dict, role_arn: str, aws_region: str) -> str:\n",
    "    return await asyncio.to_thread(deploy_model, experiment_config, role_arn, aws_region)\n",
    "\n",
    "async def async_deploy_all_models(config: Dict) -> List[Dict]:\n",
    "    experiments: List[Dict] = config['experiments']\n",
    "    n: int = 4 # max concurrency so as to not get a throttling exception\n",
    "    experiments_splitted = [experiments[i * n:(i + 1) * n] for i in range((len(experiments) + n - 1) // n )]\n",
    "    results = []\n",
    "    for exp_list in experiments_splitted:\n",
    "        result = await asyncio.gather(*[async_deploy_model(m,\n",
    "                                                           config['aws']['region'],\n",
    "                                                           config['aws']['sagemaker_execution_role']) for m in exp_list])\n",
    "        results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-17 22:37:54,101] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-70b-chat-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118', 'model_id': 'llama2-70b-chat', 'model_version': '*', 'model_name': 'llama2-70b-MODEL_chat', 'ep_name': 'llama-2-70b-chat-p4d-24xlarge', 'instance_type': 'ml.p4d.24xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'p4d_hf_tgi.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'MODEL_LOADING_TIMEOUT': '3600', 'NUMBER_OF_GPU': 8, 'INSTANCE_COUNT': 1, 'HEALTH_CHECK_TIMEOUT': 300}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:37:54,106] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-12xlarge', 'instance_type': 'ml.g5.12xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '4', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:37:54,107] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-24xlarge', 'instance_type': 'ml.g5.24xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '4', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:37:54,107] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-13b', 'model_version': '*', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-g5-48xlarge', 'instance_type': 'ml.g5.48xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '8', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:37:54,113] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\p4d_hf_tgi.py\n",
      "[2024-01-17 22:37:54,117] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\jumpstart.py\n",
      "[2024-01-17 22:37:54,117] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\jumpstart.py\n",
      "[2024-01-17 22:37:54,118] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\jumpstart.py\n",
      "[2024-01-17 22:37:55,313] p51768 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\aroraai\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-17 22:37:57,030] p51768 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "[2024-01-17 22:37:57,031] p51768 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "[2024-01-17 22:37:57,031] p51768 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region name -> us-east-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-17 22:38:00,270] p51768 {p4d_hf_tgi.py:108} INFO - deploying the model using the llm_model and the configurations ....\n",
      "[2024-01-17 22:38:00,295] p51768 {image_uris.py:579} INFO - Defaulting to only available Python version: py39\n",
      "[2024-01-17 22:38:00,312] p51768 {image_uris.py:503} INFO - Defaulting to only supported image scope: gpu.\n",
      "[2024-01-17 22:38:00,313] p51768 {p4d_hf_tgi.py:112} INFO - retrieved the inference uri -> 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n",
      "[2024-01-17 22:38:00,337] p51768 {p4d_hf_tgi.py:116} INFO - the llm_model has been defined .... <sagemaker.huggingface.model.HuggingFaceModel object at 0x000002801277BF90>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, retrieving the hugging face image uri .....\n",
      "The image uri being used -> 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n",
      "Setting the model configurations .....\n",
      "Hugging face model defined using {'HF_MODEL_ID': 'meta-llama/Llama-2-70b-chat-hf', 'SM_NUM_GPUS': '8', 'MAX_INPUT_LENGTH': '4090', 'MAX_TOTAL_TOKENS': '4096', 'MAX_BATCH_TOTAL_TOKENS': '8192', 'HUGGING_FACE_HUB_TOKEN': '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'} -> <sagemaker.huggingface.model.HuggingFaceModel object at 0x000002801277BF90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-17 22:38:00,610] p51768 {utils.py:475} INFO - Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-17 22:38:00,612] p51768 {cache.py:437} WARNING - Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-17 22:38:00,615] p51768 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-613\n",
      "Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-17 22:38:00,625] p51768 {utils.py:475} INFO - Model 'meta-textgeneration-llama-2-13b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-17 22:38:00,629] p51768 {cache.py:437} WARNING - Using model 'meta-textgeneration-llama-2-13b' with wildcard version identifier '*'. You can pin to version '3.0.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-17 22:38:00,632] p51768 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-630\n",
      "[2024-01-17 22:38:00,785] p51768 {session.py:3701} INFO - Creating model with name: meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-784\n",
      "[2024-01-17 22:38:02,022] p51768 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-24xlarge-1705549080\n",
      "[2024-01-17 22:38:02,031] p51768 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-12xlarge-1705549080\n",
      "[2024-01-17 22:38:02,238] p51768 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-g5-48xlarge-1705549080\n",
      "[2024-01-17 22:38:02,409] p51768 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-24xlarge-1705549080\n",
      "[2024-01-17 22:38:02,425] p51768 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-12xlarge-1705549080\n",
      "[2024-01-17 22:38:02,641] p51768 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-g5-48xlarge-1705549080\n",
      "[2024-01-17 22:38:02,717] p51768 {session.py:3701} INFO - Creating model with name: huggingface-pytorch-tgi-inference-2024-01-18-03-38-02-716\n",
      "[2024-01-17 22:38:07,059] p51768 {session.py:5377} INFO - Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058\n",
      "[2024-01-17 22:38:07,342] p51768 {session.py:4279} INFO - Creating endpoint with name huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------!-!---!------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-17 22:47:10,863] p51768 {p4d_hf_tgi.py:119} INFO - Deploying the model now ....\n",
      "[2024-01-17 22:47:11,585] p51768 {p4d_hf_tgi.py:122} INFO - Endpoint status: InService\n",
      "[2024-01-17 22:47:11,596] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=12', 'model_id': 'meta-textgenerationneuron-llama-2-13b-f', 'model_version': '1.0.0', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-inf2-24xlarge', 'instance_type': 'ml.inf2.24xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'OPTION_DTYPE': 'fp16', 'OPTION_MAX_ROLLING_BATCH_SIZE': '4', 'OPTION_N_POSITIONS': '4096', 'OPTION_TENSOR_PARALLEL_DEGREE': '12', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1', 'SAGEMAKER_TS_RESPONSE_TIMEOUT': '120', 'SAGEMAKER_MODEL_SERVER_TIMEOUT': '120'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:47:11,596] p51768 {2014866166.py:3} INFO - going to deploy {'name': 'llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24', 'model_id': 'meta-textgenerationneuron-llama-2-13b-f', 'model_version': '1.0.0', 'model_name': 'llama2-13b', 'ep_name': 'llama-2-13b-inf2-48xlarge', 'instance_type': 'ml.inf2.48xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'OPTION_DTYPE': 'fp16', 'OPTION_MAX_ROLLING_BATCH_SIZE': '4', 'OPTION_N_POSITIONS': '4096', 'OPTION_TENSOR_PARALLEL_DEGREE': '24', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1', 'SAGEMAKER_TS_RESPONSE_TIMEOUT': '120', 'SAGEMAKER_MODEL_SERVER_TIMEOUT': '120'}}, in us-east-1 with arn:aws:iam::015469603702:role/SageMakerRepoRole\n",
      "[2024-01-17 22:47:11,599] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\jumpstart.py\n",
      "[2024-01-17 22:47:11,600] p51768 {2014866166.py:13} INFO - going to deploy using code in C:\\Users\\aroraai\\repos\\llama-2-neuron-benchmarking\\scripts\\jumpstart.py\n",
      "Model 'meta-textgenerationneuron-llama-2-13b-f' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-17 22:47:12,436] p51768 {utils.py:475} INFO - Model 'meta-textgenerationneuron-llama-2-13b-f' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "Model 'meta-textgenerationneuron-llama-2-13b-f' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-17 22:47:12,437] p51768 {utils.py:475} INFO - Model 'meta-textgenerationneuron-llama-2-13b-f' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "[2024-01-17 22:47:12,449] p51768 {session.py:3701} INFO - Creating model with name: meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-442\n",
      "[2024-01-17 22:47:12,449] p51768 {session.py:3701} INFO - Creating model with name: meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-448\n",
      "[2024-01-17 22:47:14,087] p51768 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-inf2-48xlarge-1705549632\n",
      "[2024-01-17 22:47:14,093] p51768 {session.py:5377} INFO - Creating endpoint-config with name llama-2-13b-inf2-24xlarge-1705549632\n",
      "[2024-01-17 22:47:14,534] p51768 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-inf2-24xlarge-1705549632\n",
      "[2024-01-17 22:47:16,985] p51768 {session.py:4279} INFO - Creating endpoint with name llama-2-13b-inf2-48xlarge-1705549632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------!!endpoint_names -> [{'endpoint_name': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058', 'experiment_name': 'llama2-70b-chat-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118'}, {'endpoint_name': 'llama-2-13b-g5-12xlarge-1705549080', 'experiment_name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}, {'endpoint_name': 'llama-2-13b-g5-24xlarge-1705549080', 'experiment_name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}, {'endpoint_name': 'llama-2-13b-g5-48xlarge-1705549080', 'experiment_name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}, {'endpoint_name': 'llama-2-13b-inf2-24xlarge-1705549632', 'experiment_name': 'llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=12'}, {'endpoint_name': 'llama-2-13b-inf2-48xlarge-1705549632', 'experiment_name': 'llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24'}], deployed in 1046.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# async version\n",
    "s = time.perf_counter()\n",
    "endpoint_names = await async_deploy_all_models(config)\n",
    "elapsed_async = time.perf_counter() - s\n",
    "print(f\"endpoint_names -> {endpoint_names}, deployed in {elapsed_async:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_name': 'llama2-70b-chat-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118',\n",
       "  'endpoint': {'EndpointName': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058',\n",
       "   'EndpointConfigName': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:947399ae8b3fa131fc6d2da99f56c9c41195c7ce7cbd890e1e6c0dc328d238cd',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 38, 8, 323000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 7, 661000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 46, 50, 642000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': 'ac3b1959-59f9-4b8b-865c-587391243f83',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'ac3b1959-59f9-4b8b-865c-587391243f83',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '884',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:20 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/huggingface-pytorch-tgi-inference-2024-01-18-03-38-07-058',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-02-716',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.p4d.24xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 300}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 7, 313000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': 'f97944e4-e542-4404-9364-4e11fc80a204',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'f97944e4-e542-4404-9364-4e11fc80a204',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '541',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:20 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'huggingface-pytorch-tgi-inference-2024-01-18-03-38-02-716',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'Environment': {'HF_MODEL_ID': 'meta-llama/Llama-2-70b-chat-hf',\n",
       "     'HUGGING_FACE_HUB_TOKEN': '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',\n",
       "     'MAX_BATCH_TOTAL_TOKENS': '8192',\n",
       "     'MAX_INPUT_LENGTH': '4090',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "     'SAGEMAKER_REGION': 'us-east-1',\n",
       "     'SM_NUM_GPUS': '8'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 6, 923000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/huggingface-pytorch-tgi-inference-2024-01-18-03-38-02-716',\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': 'eb141c1a-bc80-4c5f-b51b-e76b18d324d1',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'eb141c1a-bc80-4c5f-b51b-e76b18d324d1',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '897',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:21 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-12xlarge-1705549080',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-12xlarge-1705549080',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-12xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 38, 4, 39000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 800000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 42, 55, 73000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '6cff6c3c-376a-415d-a830-23baa75bd567',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '6cff6c3c-376a-415d-a830-23baa75bd567',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '813',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:21 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-12xlarge-1705549080',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-12xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-613',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.12xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 383000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': '4310e971-585c-40a1-b903-482066eacb30',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '4310e971-585c-40a1-b903-482066eacb30',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:21 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-613',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '4'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 1, 880000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-613',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': '6cf38dcf-e55e-45b0-b779-85e2f9498a21',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '6cf38dcf-e55e-45b0-b779-85e2f9498a21',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1137',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:21 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-24xlarge-1705549080',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-24xlarge-1705549080',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-24xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 38, 3, 486000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 793000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 42, 54, 770000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': 'd73b7898-9a79-4db2-a90a-224f3ea77238',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'd73b7898-9a79-4db2-a90a-224f3ea77238',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '814',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:22 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-24xlarge-1705549080',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-24xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-630',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.24xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 366000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': '1a425ff3-e61f-4475-9eb6-712153f5cf35',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '1a425ff3-e61f-4475-9eb6-712153f5cf35',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:22 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-630',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '4'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 1, 875000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-630',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': '4f80c743-939e-44a9-a8ba-82b02436d9ce',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '4f80c743-939e-44a9-a8ba-82b02436d9ce',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1138',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:22 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-g5-48xlarge-1705549080',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-g5-48xlarge-1705549080',\n",
       "   'EndpointConfigName': 'llama-2-13b-g5-48xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 38, 4, 38000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 3, 30000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 43, 42, 382000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '734f89d4-9dc6-4f25-92e6-4c6b631b1c75',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '734f89d4-9dc6-4f25-92e6-4c6b631b1c75',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '814',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:23 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-g5-48xlarge-1705549080',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-g5-48xlarge-1705549080',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-784',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.48xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 594000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': '8afb06a8-51c7-4b24-b56f-1ec678a396e2',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '8afb06a8-51c7-4b24-b56f-1ec678a396e2',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '534',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:23 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-784',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-13b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_INPUT_LENGTH': '4095',\n",
       "     'MAX_TOTAL_TOKENS': '4096',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '8'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 38, 2, 35000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-13b-2024-01-18-03-38-00-784',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': 'dbadfc60-d0ab-4de8-b833-18de195c6db1',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'dbadfc60-d0ab-4de8-b833-18de195c6db1',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1138',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:23 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=12',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-inf2-24xlarge-1705549632',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-inf2-24xlarge-1705549632',\n",
       "   'EndpointConfigName': 'llama-2-13b-inf2-24xlarge-1705549632',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference@sha256:73a0c250ab43b4a89171187fa0e6bbbaac029cf652207798af17dd9b30546ec6',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 47, 15, 815000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 14, 929000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 54, 58, 981000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': '84ab5109-683d-41c1-a349-6e9a0f22e5c0',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '84ab5109-683d-41c1-a349-6e9a0f22e5c0',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '764',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:24 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-inf2-24xlarge-1705549632',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-inf2-24xlarge-1705549632',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-442',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.inf2.24xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'VolumeSizeInGB': 256,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 3600,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 3600}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 14, 493000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': 'a553b327-be3f-4111-ae97-6a32950e497f',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'a553b327-be3f-4111-ae97-6a32950e497f',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '569',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:24 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-442',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgenerationneuron/meta-textgenerationneuron-llama-2-13b-f/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'OPTION_DTYPE': 'fp16',\n",
       "     'OPTION_MAX_ROLLING_BATCH_SIZE': '4',\n",
       "     'OPTION_NEURON_OPTIMIZE_LEVEL': '2',\n",
       "     'OPTION_N_POSITIONS': '4096',\n",
       "     'OPTION_ROLLING_BATCH': 'auto',\n",
       "     'OPTION_TENSOR_PARALLEL_DEGREE': '12',\n",
       "     'SAGEMAKER_MODEL_SERVER_TIMEOUT': '120',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SAGEMAKER_TS_RESPONSE_TIMEOUT': '120'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 13, 941000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-442',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': 'a524071e-c598-4c6c-8007-d6e600802bfb',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'a524071e-c598-4c6c-8007-d6e600802bfb',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1208',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:24 GMT'},\n",
       "    'RetryAttempts': 0}}},\n",
       " {'experiment_name': 'llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24',\n",
       "  'endpoint': {'EndpointName': 'llama-2-13b-inf2-48xlarge-1705549632',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-13b-inf2-48xlarge-1705549632',\n",
       "   'EndpointConfigName': 'llama-2-13b-inf2-48xlarge-1705549632',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference@sha256:73a0c250ab43b4a89171187fa0e6bbbaac029cf652207798af17dd9b30546ec6',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 17, 22, 47, 18, 20000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 17, 364000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 17, 22, 54, 57, 249000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': 'e9c74157-b18f-4d2b-a1b6-27d1a49a0468',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'e9c74157-b18f-4d2b-a1b6-27d1a49a0468',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '763',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:25 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'llama-2-13b-inf2-48xlarge-1705549632',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-13b-inf2-48xlarge-1705549632',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-448',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.inf2.48xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'VolumeSizeInGB': 256,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 3600,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 3600}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 16, 903000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': 'afe922a3-408b-45b9-a9c6-33909358482f',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'afe922a3-408b-45b9-a9c6-33909358482f',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '569',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:26 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-448',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgenerationneuron/meta-textgenerationneuron-llama-2-13b-f/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'OPTION_DTYPE': 'fp16',\n",
       "     'OPTION_MAX_ROLLING_BATCH_SIZE': '4',\n",
       "     'OPTION_NEURON_OPTIMIZE_LEVEL': '2',\n",
       "     'OPTION_N_POSITIONS': '4096',\n",
       "     'OPTION_ROLLING_BATCH': 'auto',\n",
       "     'OPTION_TENSOR_PARALLEL_DEGREE': '24',\n",
       "     'SAGEMAKER_MODEL_SERVER_TIMEOUT': '120',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SAGEMAKER_TS_RESPONSE_TIMEOUT': '120'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/SageMakerRepoRole',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 17, 22, 47, 13, 937000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgenerationneuron-llama-2-13b-f-2024-01-18-03-47-12-448',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': 'c676683e-a1ca-4160-93cc-f5ccf1da7e2f',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'c676683e-a1ca-4160-93cc-f5ccf1da7e2f',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1208',\n",
       "     'date': 'Thu, 18 Jan 2024 03:55:26 GMT'},\n",
       "    'RetryAttempts': 0}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_info_for_endpoint(ep: Dict) -> Dict:\n",
    "    ep_name = ep['endpoint_name']\n",
    "    experiment_name = ep['experiment_name']\n",
    "    if ep_name is None:\n",
    "        return None\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    endpoint = sm_client.describe_endpoint(EndpointName=ep_name)\n",
    "    endpoint_config = sm_client.describe_endpoint_config(EndpointConfigName=endpoint['EndpointConfigName'])\n",
    "    model_config = sm_client.describe_model(ModelName=endpoint_config['ProductionVariants'][0]['ModelName'])\n",
    "    info = dict(experiment_name=experiment_name,\n",
    "                endpoint=endpoint,\n",
    "                endpoint_config=endpoint_config,\n",
    "                model_config=model_config)\n",
    "    return info\n",
    "\n",
    "all_info = list(map(get_all_info_for_endpoint, [ep for ep in endpoint_names if ep is not None]))\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28176"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write all end point info to a file so that other notebooks can read it\n",
    "Path(ENDPOINT_LIST_FPATH).write_text(json.dumps(all_info, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
