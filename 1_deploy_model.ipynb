{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Jumpstart and Non Jumpstart Models Asynchronously \n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "**This step of our solution design covers setting up the environment, downloading the requirements needed to run the environment, as well as deploying the model endpoints from the config.yml file asychronously.**\n",
    "\n",
    "1. Prerequisite: Navigate to the file: 0_setup.ipynb and Run the cell to import and download the requirements.txt.\n",
    "\n",
    "2. Now you can run this notebook to deploy the models asychronously in different threads. The key components of this notebook for the purposes of understanding are:\n",
    "\n",
    "- Loading the globals.py and config.yml file.\n",
    "\n",
    "- Setting a blocker function deploy_model to deploy the given model endpoint followed by:\n",
    "\n",
    "- A series of async functions to set tasks to deploy the models from the config yml file asynchronously in different threads. View the notebook from the link above.\n",
    "\n",
    "- Once the endpoints are deployed, their model configurations are stored within the endpoints.json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!touch globals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import pathlib\n",
    "import importlib.util\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from globals import *\n",
    "from pathlib import Path\n",
    "from utils import load_config\n",
    "from sagemaker import get_execution_role\n",
    "from utils import write_to_s3 \n",
    "from typing import Dict, List, Optional\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# S3 client initialization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompts_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metrics/yyyy={yyyy}/mm={mm}/dd={dd}/hh={hh}/name={name}/ --> how all s3 prefixes should be defined\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "current_time = datetime.now()\u001b[37m\u001b[39;49;00m\n",
      "formatted_time = current_time.strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm/\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/metrics/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformatted_time\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# MODELS_DIR = os.path.join(DATA_DIR, \"models\", config['general']['name'])\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/models/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformatted_time\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## DEFINE THE S3 PATH FOR ENDPOINTS TO READ FROM DURING RUN INFERENCE\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_S3_PATH = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mMODELS_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/endpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Use this to upload to the s3 bucket (extracted from the config file)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUCKET_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## S3 prefix\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PREFIX_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## SOURCE data is where your actual data resides in s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SOURCE_DATA = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msource_data_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Read the prompt template that the user uploads\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPT_TEMPLATE_S3_PREFIX = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompt_template_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# TOKENIZER_DIR = 'llama2_tokenizer'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## this is for custom tokenizers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR_S3 = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mtokenizer_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER = \u001b[33m'\u001b[39;49;00m\u001b[33mtokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DEPLOYMENT_SCRIPT_S3 = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mscripts_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config.yml file\n",
    "------\n",
    "\n",
    "The config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations, and model configurations like the version of the model, the endpoint name, model_id that needs to be deployed. Configurations also support the gives instance type to be used, for example: \"ml.g5.24xlarge\", the image uri, whether or not to deploy this given model, followed by an inference script \"jumpstart.py\" which supports the inference script for jumpstart models to deploy the model in this deploy notebook. \n",
    "\n",
    "View the contents of the config yml file below and how it is loaded and used throughout this notebook with deploying the model endpoints asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 15:48:06,174] p15748 {session.py:5460} WARNING - Couldn't call 'get_role' to get Role ARN from role name madhurusertest to get Role path.\n",
      "[2024-01-29 15:48:06,175] p15748 {2048960905.py:13} ERROR - could not determine SageMaker execution role, error=The current AWS identity is not a role: arn:aws:iam::015469603702:user/madhurusertest, therefore it cannot be used as a SageMaker execution role\n",
      "[2024-01-29 15:48:06,176] p15748 {2048960905.py:14} INFO - going to look for execution role in config file..\n",
      "[2024-01-29 15:48:06,176] p15748 {2048960905.py:17} INFO - found SageMaker execution role in config file..\n",
      "[2024-01-29 15:48:06,176] p15748 {2048960905.py:19} INFO - aws_region=us-east-1, sagemaker_execution_role=arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20220504T122644\n",
      "[2024-01-29 15:48:06,177] p15748 {2048960905.py:20} INFO - config={\n",
      "  \"general\": {\n",
      "    \"name\": \"mistral-7b-tgi-g5-v1\",\n",
      "    \"model_name\": \"mistral7b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20220504T122644\",\n",
      "    \"bucket\": \"fmbttest\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics_{datetime}\",\n",
      "    \"models_dir\": \"models_{datetime}\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_500-1000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.2xlarge\": 1.515,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"huggingface-llm-mistral-7b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"mistral7b\",\n",
      "      \"ep_name\": \"lmistral7b-g5-2xlarge\",\n",
      "      \"instance_type\": \"ml.g5.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "        \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "        \"SM_NUM_GPUS\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Load the config.yml file referring to the globals.py file\n",
    "config = load_config(CONFIG_FILE)\n",
    "\n",
    "## configure the aws region and execution role\n",
    "aws_region = config['aws']['region']\n",
    "\n",
    "\n",
    "try:\n",
    "    sagemaker_execution_role = get_execution_role()\n",
    "    config['aws']['sagemaker_execution_role'] = sagemaker_execution_role\n",
    "    logger.info(f\"determined SageMaker exeuction role from get_execution_role\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"could not determine SageMaker execution role, error={e}\")\n",
    "    logger.info(f\"going to look for execution role in config file..\")\n",
    "    sagemaker_execution_role = config['aws'].get('sagemaker_execution_role')\n",
    "    if sagemaker_execution_role is not None:\n",
    "        logger.info(f\"found SageMaker execution role in config file..\")\n",
    "\n",
    "logger.info(f\"aws_region={aws_region}, sagemaker_execution_role={sagemaker_execution_role}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy a single model: blocker function used for asynchronous deployment\n",
    "\n",
    "This function is designed to deploy a single large language model endpoint. It takes three parameters: experiment_config (a dictionary containing configuration details for the model deployment from the config.yml file), aws_region (the AWS region where the model will be deployed), and role_arn (the AWS role's Amazon Resource Name used for the deployment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to deploy a model\n",
    "def deploy_model(experiment_config: Dict, aws_region: str, role_arn: str) -> Optional[Dict]:\n",
    "    \n",
    "    # Log the deployment details\n",
    "    logger.info(f\"going to deploy {experiment_config}, in {aws_region} with {role_arn}\")\n",
    "    model_deployment_result = None\n",
    "    \n",
    "    # Check if deployment is enabled in the config; skip if not\n",
    "    deploy = experiment_config.get('deploy', False)\n",
    "    if deploy is False:\n",
    "        logger.error(f\"skipping deployment of {experiment_config['model_id']} because deploy={deploy}\")\n",
    "        return model_deployment_result\n",
    "    \n",
    "    try: \n",
    "        # Set up the module name and S3 path\n",
    "        module_name = Path(experiment_config['deployment_script']).stem\n",
    "        logger.info(f\"script name being used for model deployment --> {module_name}\")\n",
    "\n",
    "        ## Defining the s3 path to pull out the script for your specific model\n",
    "        s3_file_path = f\"{DEPLOYMENT_SCRIPT_S3}/{module_name}.py\"\n",
    "        logger.info(f\"s3 file path where your model script should be installed in --> {s3_file_path}\")\n",
    "\n",
    "        # Check if the module exists in S3 to make sure that you have uploaded it in\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            file_exists_in_s3 = True\n",
    "        except ClientError:\n",
    "            file_exists_in_s3 = False\n",
    "\n",
    "        # Define the local script path where we will install this script and use it to deploy the model\n",
    "        local_script_path = f\"/tmp/{module_name}.py\"\n",
    "\n",
    "        if file_exists_in_s3:\n",
    "            # Download the script from S3\n",
    "            s3_client.download_file(config['aws']['bucket'], s3_file_path, local_script_path)\n",
    "            logger.info(f\"Deploying using code from S3: {local_script_path}\")\n",
    "        else:\n",
    "            # Fall back to local file\n",
    "            local_script_path = os.path.join(pathlib.Path().absolute().resolve(), SCRIPTS_DIR, f\"{module_name}.py\")\n",
    "            logger.info(f\"Deploying using local code: {local_script_path}\")\n",
    "\n",
    "        # Load the specific module name and file path\n",
    "        spec = importlib.util.spec_from_file_location(module_name, local_script_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "\n",
    "        # Execute the deployment function from the imported module\n",
    "        model_deployment_result = module.deploy(experiment_config, role_arn)\n",
    "\n",
    "        # Return the model deployment result \n",
    "        return model_deployment_result\n",
    "\n",
    "    \n",
    "    except ClientError as error:\n",
    "        print(f\"an error occurred: {error}\")\n",
    "        return model_deployment_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Model Deployment\n",
    "----\n",
    "\n",
    "#### async_deploy_model: \n",
    "\n",
    "- This is an asynchronous wrapper around the deploy_model function. It uses asyncio.to_thread to run the synchronous deploy_model function in a separate thread. This allows the function to be awaited in an asynchronous context, enabling concurrent model deployments without any blocking from the main thread\n",
    "\n",
    "#### async_deploy_all_models Function: \n",
    "\n",
    "- This 'async_deploy_all_models' function is designed to deploy multiple models concurrently. It splits the models into batches and deploys each batch concurrently using asyncio.gather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Asynchronous wrapper function to allow our deploy_model function to allow concurrent requests for deployment\n",
    "async def async_deploy_model(experiment_config: Dict, role_arn: str, aws_region: str) -> str:\n",
    "    # Run the deploy_model function in a separate thread to deploy the models asychronously\n",
    "    return await asyncio.to_thread(deploy_model, experiment_config, role_arn, aws_region)\n",
    "\n",
    "## Final asychronous function to deploy all of the models concurrently\n",
    "async def async_deploy_all_models(config: Dict) -> List[Dict]:\n",
    "    \n",
    "    ## Extract experiments from the config.yml file (contains information on model configurations)\n",
    "    experiments: List[Dict] = config['experiments']\n",
    "    n: int = 4 # max concurrency so as to not get a throttling exception\n",
    "    \n",
    "    ## Split experiments into smaller batches for concurrent deployment\n",
    "    experiments_splitted = [experiments[i * n:(i + 1) * n] for i in range((len(experiments) + n - 1) // n )]\n",
    "    results = []\n",
    "    for exp_list in experiments_splitted:\n",
    "        \n",
    "        ## send the deployment in batches\n",
    "        result = await asyncio.gather(*[async_deploy_model(m,\n",
    "                                                           config['aws']['region'],\n",
    "                                                           config['aws']['sagemaker_execution_role']) for m in exp_list])\n",
    "        ## Collect and furthermore extend the results from each batch\n",
    "        results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 15:48:06,386] p15748 {2611178736.py:5} INFO - going to deploy {'name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'huggingface-llm-mistral-7b', 'model_version': '*', 'model_name': 'mistral7b', 'ep_name': 'lmistral7b-g5-2xlarge', 'instance_type': 'ml.g5.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '8191', 'MAX_TOTAL_TOKENS': '8192', 'MAX_BATCH_PREFILL_TOKENS': '8191', 'SM_NUM_GPUS': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, in us-east-1 with arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20220504T122644\n",
      "[2024-01-29 15:48:06,387] p15748 {2611178736.py:17} INFO - script name being used for model deployment --> jumpstart\n",
      "[2024-01-29 15:48:06,387] p15748 {2611178736.py:21} INFO - s3 file path where your model script should be installed in --> scripts/jumpstart.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 15:48:06,562] p15748 {2611178736.py:40} INFO - Deploying using local code: /Users/madhurpt/fmbt/scripts/jumpstart.py\n",
      "Using model 'huggingface-llm-mistral-7b' with wildcard version identifier '*'. You can pin to version '2.0.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-29 15:48:07,125] p15748 {cache.py:437} WARNING - Using model 'huggingface-llm-mistral-7b' with wildcard version identifier '*'. You can pin to version '2.0.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-01-29 15:48:07,127] p15748 {session.py:3701} INFO - Creating model with name: hf-llm-mistral-7b-2024-01-29-20-48-07-126\n",
      "[2024-01-29 15:48:07,954] p15748 {session.py:5377} INFO - Creating endpoint-config with name lmistral7b-g5-2xlarge-1706561287\n",
      "[2024-01-29 15:48:08,488] p15748 {session.py:4279} INFO - Creating endpoint with name lmistral7b-g5-2xlarge-1706561287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!endpoint_names -> [{'endpoint_name': 'lmistral7b-g5-2xlarge-1706561287', 'experiment_name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0'}], deployed in 303.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# async version\n",
    "s = time.perf_counter()\n",
    "\n",
    "## Call all of the models for deployment using the config.yml file model configurations\n",
    "endpoint_names = await async_deploy_all_models(config)\n",
    "\n",
    "## Set a timer for model deployment counter\n",
    "elapsed_async = time.perf_counter() - s\n",
    "print(f\"endpoint_names -> {endpoint_names}, deployed in {elapsed_async:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0',\n",
       "  'endpoint': {'EndpointName': 'lmistral7b-g5-2xlarge-1706561287',\n",
       "   'EndpointArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint/lmistral7b-g5-2xlarge-1706561287',\n",
       "   'EndpointConfigName': 'lmistral7b-g5-2xlarge-1706561287',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "       'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb',\n",
       "       'ResolutionTime': datetime.datetime(2024, 1, 29, 15, 48, 9, 829000, tzinfo=tzlocal())}],\n",
       "     'CurrentWeight': 1.0,\n",
       "     'DesiredWeight': 1.0,\n",
       "     'CurrentInstanceCount': 1,\n",
       "     'DesiredInstanceCount': 1}],\n",
       "   'EndpointStatus': 'InService',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 29, 15, 48, 8, 947000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2024, 1, 29, 15, 53, 6, 603000, tzinfo=tzlocal()),\n",
       "   'ResponseMetadata': {'RequestId': 'e6f0252c-55a8-4a7c-a678-eb5dd09dd836',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': 'e6f0252c-55a8-4a7c-a678-eb5dd09dd836',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '809',\n",
       "     'date': 'Mon, 29 Jan 2024 21:14:57 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'endpoint_config': {'EndpointConfigName': 'lmistral7b-g5-2xlarge-1706561287',\n",
       "   'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/lmistral7b-g5-2xlarge-1706561287',\n",
       "   'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "     'ModelName': 'hf-llm-mistral-7b-2024-01-29-20-48-07-126',\n",
       "     'InitialInstanceCount': 1,\n",
       "     'InstanceType': 'ml.g5.2xlarge',\n",
       "     'InitialVariantWeight': 1.0,\n",
       "     'ModelDataDownloadTimeoutInSeconds': 1200,\n",
       "     'ContainerStartupHealthCheckTimeoutInSeconds': 1200}],\n",
       "   'CreationTime': datetime.datetime(2024, 1, 29, 15, 48, 8, 476000, tzinfo=tzlocal()),\n",
       "   'EnableNetworkIsolation': False,\n",
       "   'ResponseMetadata': {'RequestId': '4565d13f-2048-4394-bbaa-6d0682e63eba',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '4565d13f-2048-4394-bbaa-6d0682e63eba',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '515',\n",
       "     'date': 'Mon, 29 Jan 2024 21:14:57 GMT'},\n",
       "    'RetryAttempts': 0}},\n",
       "  'model_config': {'ModelName': 'hf-llm-mistral-7b-2024-01-29-20-48-07-126',\n",
       "   'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04',\n",
       "    'Mode': 'SingleModel',\n",
       "    'ModelDataSource': {'S3DataSource': {'S3Uri': 's3://jumpstart-cache-prod-us-east-1/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference-prepack/v1.0.0/',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'CompressionType': 'None',\n",
       "      'ModelAccessConfig': {'AcceptEula': True}}},\n",
       "    'Environment': {'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "     'HF_MODEL_ID': '/opt/ml/model',\n",
       "     'MAX_BATCH_PREFILL_TOKENS': '8191',\n",
       "     'MAX_INPUT_LENGTH': '8191',\n",
       "     'MAX_TOTAL_TOKENS': '8192',\n",
       "     'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "     'SAGEMAKER_ENV': '1',\n",
       "     'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SM_NUM_GPUS': '1'}},\n",
       "   'ExecutionRoleArn': 'arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20220504T122644',\n",
       "   'CreationTime': datetime.datetime(2024, 1, 29, 15, 48, 7, 808000, tzinfo=tzlocal()),\n",
       "   'ModelArn': 'arn:aws:sagemaker:us-east-1:015469603702:model/hf-llm-mistral-7b-2024-01-29-20-48-07-126',\n",
       "   'EnableNetworkIsolation': True,\n",
       "   'DeploymentRecommendation': {'RecommendationStatus': 'COMPLETED',\n",
       "    'RealTimeInferenceRecommendations': []},\n",
       "   'ResponseMetadata': {'RequestId': '1f3535c5-cf14-4fcb-94a6-17479273fa30',\n",
       "    'HTTPStatusCode': 200,\n",
       "    'HTTPHeaders': {'x-amzn-requestid': '1f3535c5-cf14-4fcb-94a6-17479273fa30',\n",
       "     'content-type': 'application/x-amz-json-1.1',\n",
       "     'content-length': '1168',\n",
       "     'date': 'Mon, 29 Jan 2024 21:14:57 GMT'},\n",
       "    'RetryAttempts': 0}}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Function to get all of the information on the deployed endpoints and store it in a json\n",
    "def get_all_info_for_endpoint(ep: Dict) -> Dict:\n",
    "    \n",
    "    ## extract the endpoint name\n",
    "    ep_name = ep['endpoint_name']\n",
    "    \n",
    "    ## extract the experiment name from the config.yml file\n",
    "    experiment_name = ep['experiment_name']\n",
    "    if ep_name is None:\n",
    "        return None\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    ## get the description on the configuration of the deployed model\n",
    "    endpoint = sm_client.describe_endpoint(EndpointName=ep_name)\n",
    "    endpoint_config = sm_client.describe_endpoint_config(EndpointConfigName=endpoint['EndpointConfigName'])\n",
    "    model_config = sm_client.describe_model(ModelName=endpoint_config['ProductionVariants'][0]['ModelName'])\n",
    "    \n",
    "    ## Store the experiment name and all of the other model configuration information in the 'info' dict\n",
    "    info = dict(experiment_name=experiment_name,\n",
    "                endpoint=endpoint,\n",
    "                endpoint_config=endpoint_config,\n",
    "                model_config=model_config)\n",
    "    return info\n",
    "\n",
    "all_info = list(map(get_all_info_for_endpoint, [ep for ep in endpoint_names if ep is not None]))\n",
    "\n",
    "## stores information in a dictionary for collectively all of the deployed model endpoints\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/models/2024/01/29/16/mistral-7b-tgi-g5-v1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbttest/data/models/2024/01/29/16/mistral-7b-tgi-g5-v1/endpoints.json\n"
     ]
    }
   ],
   "source": [
    "# Convert data to JSON\n",
    "json_data = json.dumps(all_info, indent=2, default=str)\n",
    "\n",
    "# Extract the model name from the config\n",
    "model_name = config['general']['name']\n",
    "\n",
    "# Specify the file name\n",
    "file_name = \"endpoints.json\"\n",
    "\n",
    "# Write to S3\n",
    "write_to_s3(json_data, config['aws']['bucket'], MODELS_DIR, \"\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write all end point info to an endpoints.json file so that other notebooks can read it and make inferences from it\n",
    "# Path(ENDPOINT_LIST_FPATH).write_text(json.dumps(all_info, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
