{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and store the prompts\n",
    "\n",
    "1. Download the LLaMA 2 Tokenzier from https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main \n",
    "   and place the files into a directory named `llama2_tokenizer` in the same \n",
    "   directory as this notebook.\n",
    "\n",
    "2. install the python packages below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import logging\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from typing import Dict, List\n",
    "from utils import process_item, load_config, count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "from enum import Enum\n",
      "from pathlib import Path\n",
      "\n",
      "CONFIG_FILE: str = \"config.yml\"\n",
      "DATA_DIR: str = \"data\"\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \"prompts\")\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \"metrics\")\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \"models\")\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \"dataset\")\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR]\n",
      "TOKENIZER_DIR = 'llama2_tokenizer'\n",
      "\n",
      "_ = list(map(lambda x: os.makedirs(x, exist_ok=True), DIR_LIST))\n",
      "\n",
      "ENDPOINT_LIST_FPATH:str = os.path.join(MODELS_DIR, \"endpoints.json\")\n",
      "REQUEST_PAYLOAD_FPATH:str = os.path.join(PROMPTS_DIR, \"payload.jsonl\")\n",
      "RESULTS_FPATH:str = os.path.join(METRICS_DIR, \"results.csv\")\n",
      "class TRUNCATE_POLICY(str, Enum):\n",
      "    AT_PROMPT_TOKEN_LENGTH = 'at-prompt-token-length'    \n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-05 07:37:55,987] p44384 {635462509.py:2} INFO - {\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::015469603702:role/SageMakerRepoRole\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.7,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-conc=1-bs=4-tpd=12\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-24xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"12\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-conc=1-bs=4-tpd=24\",\n",
      "      \"model_id\": \"meta-textgenerationneuron-llama-2-13b-f\",\n",
      "      \"model_version\": \"1.0.0\",\n",
      "      \"model_name\": \"llama2-13b\",\n",
      "      \"ep_name\": \"llama-2-13b-inf2-48xlarge\",\n",
      "      \"instance_type\": \"ml.inf2.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\",\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": 1,\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"OPTION_DTYPE\": \"fp16\",\n",
      "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
      "        \"OPTION_N_POSITIONS\": \"4096\",\n",
      "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\": \"120\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"120\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"results_per_inference_request_{datetime}.csv\",\n",
      "    \"all_experiments_file\": \"results_all_experiments_{datetime}.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-05 07:16:12,844] p44384 {3431602775.py:3} INFO - prompt template from data\\prompts\\prompt_template.txt ->\n",
      "<s>[INST] <<SYS>>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "<</SYS>>\n",
      "\n",
      "```\n",
      "{context}\n",
      "```\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "[/INST]\n",
      "Answer:\n",
      "[2024-01-05 07:16:12,979] p44384 {3431602775.py:5} INFO - prompt template length=97 tokens\n"
     ]
    }
   ],
   "source": [
    "fpath: str = os.path.join(PROMPTS_DIR, config['prompt']['template_file'])\n",
    "prompt_template:str = Path(fpath).read_text().strip()\n",
    "logger.info(f\"prompt template from {fpath} ->\\n{prompt_template}\")\n",
    "empty_prompt_len_in_tokens = count_tokens(prompt_template.format(context=\"\", question=\"\"))\n",
    "logger.info(f\"prompt template length={empty_prompt_len_in_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob(os.path.join(DATASET_DIR, \"*\"))\n",
    "logger.info(f\"dataset files = {data_files}\")\n",
    "\n",
    "df = pd.concat(map(lambda f: pd.read_json(f, lines=True), data_files))\n",
    "\n",
    "logger.info(f\"dataset read from {data_files}\\nhas shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"distribution of the length field in the dataset is as follows ->\\n{df.length.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['prompt'] = df.apply(lambda row: process_item(row, prompt_template), axis=1)\n",
    "df['prompt_len'] = df.prompt.map(lambda x: x['prompt_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath: str = os.path.join(PROMPTS_DIR, config['prompt']['all_prompts_file'])\n",
    "logger.info(f\"all prompts dataframe of shape {df.shape} saved to {fpath}\")\n",
    "df.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the prompts into payload we can send to the model\n",
    "def construct_request_payload(row, config: Dict) -> Dict:\n",
    "    parameters = copy.deepcopy(config['inference_parameters'])\n",
    "    if parameters['truncate'] == TRUNCATE_POLICY.AT_PROMPT_TOKEN_LENGTH:\n",
    "        parameters['truncate'] = row['prompt_len']\n",
    "    return dict(inputs=row['prompt']['prompt'], parameters=parameters)\n",
    "\n",
    "def create_dataset_payload_file(df: pd.DataFrame, dataset_info: Dict, config: Dict) -> str:\n",
    "    logger.info(f\"going to create a payload file as dataset_info={json.dumps(dataset_info, indent=2)}\")\n",
    "    df['prompt_len_in_range'] = df.prompt.map(lambda x: x['prompt_len'] >= dataset_info['min_length_in_tokens'] and \\\n",
    "                                                        x['prompt_len'] <= dataset_info['max_length_in_tokens'])\n",
    "    # select prompts between pre-configured threshold lengths and are in the selected language\n",
    "    df_filtered = df[(df.language == dataset_info['language']) & (df.prompt_len_in_range)]\n",
    "    logger.info(f\"after filtering for {json.dumps(dataset_info, indent=2)}, shape of dataframe is {df_filtered.shape}\")\n",
    "    # df_filtered.head()\n",
    "\n",
    "    df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
    "    logger.info(f\"payload request entry looks like this -> {json.dumps(df_filtered['request'].iloc[0], indent=2)}\")\n",
    "    \n",
    "    # save to the payload file\n",
    "    lang = dataset_info['language']\n",
    "    min = dataset_info['min_length_in_tokens']\n",
    "    max = dataset_info['max_length_in_tokens']\n",
    "    fpath: str = os.path.join(PROMPTS_DIR, dataset_info['payload_file'].format(lang=lang, min=min, max=max))\n",
    "    logger.info(f\"creating payload file={fpath}\")\n",
    "\n",
    "    # write the requests to a jsonl file\n",
    "    df_filtered['request'].to_json(fpath, orient='records', lines=True)\n",
    "    logger.info(f\"dataset saved to {fpath}\")\n",
    "    return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ((df, d, config) for d in config['datasets'])\n",
    "paths: List = list(itertools.starmap(create_dataset_payload_file, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
