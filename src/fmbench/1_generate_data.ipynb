{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data: Gather data, create prompts/payloads of different sizes\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes \n",
    "\n",
    "- running and downloading our specific dataset\n",
    "\n",
    "- generating prompts as payloads of different sizes that we will send to our different model endpoints with different combinations of concurrency levels that we will later use to run inference and generate benchmarking metrics and visualizations.\n",
    "\n",
    "#### This file will generate all data on wikiqa (english version) with prompt sizes 300 - 4000 token lengths in different payload sizes to send to the model endpoint during the inference pipeline. You will also be able to generate the normal wikiqa dataset from the actual 'long bench dataset'. This notebook then focuses on 3 main deliverables:\n",
    "\n",
    "1. Loading the dataset that is stored within the dataset in the data directory.\n",
    "\n",
    "\n",
    "2. Generating payloads: This notebook also converts the loaded datasets into payloads based on the input question and records teh context length of the prompt to send as a part of the payload during running inferences on the deployed endpoints.\n",
    "\n",
    "    - All of the prompts are saved in this data directory in a file named all_prompts.csv.\n",
    "    \n",
    "\n",
    "3. Constructing different sized payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file current -> configs/config-distilbert-base-uncased.yml, None\n",
      "Loaded config: {'general': {'name': 'distilbert-base-uncased-v1', 'model_name': 'distilbert-base-uncased'}, 'aws': {'region': 'us-east-1', 'sagemaker_execution_role': 'arn:aws:sts::015469603702:assumed-role/fmbench-role/SageMaker', 'bucket': 'sagemaker-fmbench-write-015469603702'}, 'dir_paths': {'data_prefix': 'data', 'prompts_prefix': 'prompts', 'all_prompts_file': 'all_prompts.csv', 'metrics_dir': 'metrics', 'models_dir': 'models', 'metadata_dir': 'metadata'}, 's3_read_data': {'read_bucket': 'sagemaker-fmbench-read-015469603702', 'scripts_prefix': 'scripts', 'script_files': ['hf_token.txt'], 'source_data_prefix': 'source_data', 'tokenizer_prefix': 'tokenizer', 'prompt_template_dir': 'prompt_template', 'prompt_template_file': 'prompt_template.txt'}, 'run_steps': {'0_setup.ipynb': True, '1_generate_data.ipynb': True, '2_deploy_model.ipynb': True, '3_run_inference.ipynb': True, '4_model_metric_analysis.ipynb': True, '5_cleanup.ipynb': True}, 'datasets': {'prompt_template_keys': ['text'], 'filters': [{'language': 'en', 'min_length_in_tokens': 1, 'max_length_in_tokens': 500, 'payload_file': 'payload_en_1-500.jsonl'}, {'language': 'en', 'min_length_in_tokens': 500, 'max_length_in_tokens': 1000, 'payload_file': 'payload_en_500-1000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 1000, 'max_length_in_tokens': 2000, 'payload_file': 'payload_en_1000-2000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 2000, 'max_length_in_tokens': 3000, 'payload_file': 'payload_en_2000-3000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 3000, 'max_length_in_tokens': 4000, 'payload_file': 'payload_en_3000-4000.jsonl'}, {'language': 'en', 'min_length_in_tokens': 305, 'max_length_in_tokens': 3997, 'payload_file': 'payload_en_305-3997.jsonl'}, {'language': 'en', 'min_length_in_tokens': 100, 'max_length_in_tokens': 250, 'payload_file': 'payload_en_100-250.jsonl'}, {'language': 'en', 'min_length_in_tokens': 10, 'max_length_in_tokens': 20, 'payload_file': 'payload_en_10-20.jsonl'}, {'language': 'en', 'min_length_in_tokens': 50, 'max_length_in_tokens': 150, 'payload_file': 'payload_en_50-150.jsonl'}]}, 'metrics': {'dataset_of_interest': 'en_50-150', 'weights': {'price_per_tx_wt': 0.65, 'latenct_wt': 0.35}}, 'pricing': {'ml.m5.xlarge': 0.23, 'ml.g5.xlarge': 1.006, 'ml.g5.2xlarge': 1.212, 'ml.g5.12xlarge': 7.09, 'ml.g5.24xlarge': 10.18, 'ml.g5.48xlarge': 20.36, 'ml.inf2.24xlarge': 7.79, 'ml.inf2.48xlarge': 15.58, 'ml.p4d.24xlarge': 37.688, 'ml.p3.2xlarge': 3.825}, 'inference_parameters': {'ContentType': 'application/x-text', 'Accept': 'application/json;verbose'}, 'experiments': [{'name': 'distilbert-base-uncased-ml-p3-2xlarge', 'model_id': 'huggingface-tc-distilbert-base-uncased', 'model_version': '*', 'model_name': 'distilbert-base-uncased', 'ep_name': 'distilbert-base-uncased', 'instance_type': 'ml.p3.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'inference_script': 'sagemaker_predictor.py', 'payload_files': ['payload_en_50-150.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}, {'name': 'distilbert-base-uncased-ml-m5-xlarge', 'model_id': 'huggingface-tc-distilbert-base-uncased', 'model_version': '*', 'model_name': 'distilbert-base-uncased', 'ep_name': 'distilbert-base-uncased', 'instance_type': 'ml.m5.xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'inference_script': 'sagemaker_predictor.py', 'payload_files': ['payload_en_50-150.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}], 'report': {'per_inference_request_file': 'per_inference_request_results.csv', 'all_metrics_file': 'all_metrics.csv', 'txn_count_for_showing_cost': 100000, 'v_shift_w_single_instance': 0.025, 'v_shift_w_gt_one_instance': 0.025}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTokenizer, based on HF transformers\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:52,656] p30320 {2001288519.py:4} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"distilbert-base-uncased-v1\",\n",
      "    \"model_name\": \"distilbert-base-uncased\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:sts::015469603702:assumed-role/fmbench-role/SageMaker\",\n",
      "    \"bucket\": \"sagemaker-fmbench-write-015469603702\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics\",\n",
      "    \"models_dir\": \"models\",\n",
      "    \"metadata_dir\": \"metadata\"\n",
      "  },\n",
      "  \"s3_read_data\": {\n",
      "    \"read_bucket\": \"sagemaker-fmbench-read-015469603702\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"script_files\": [\n",
      "      \"hf_token.txt\"\n",
      "    ],\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"0_setup.ipynb\": true,\n",
      "    \"1_generate_data.ipynb\": true,\n",
      "    \"2_deploy_model.ipynb\": true,\n",
      "    \"3_run_inference.ipynb\": true,\n",
      "    \"4_model_metric_analysis.ipynb\": true,\n",
      "    \"5_cleanup.ipynb\": true\n",
      "  },\n",
      "  \"datasets\": {\n",
      "    \"prompt_template_keys\": [\n",
      "      \"text\"\n",
      "    ],\n",
      "    \"filters\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1,\n",
      "        \"max_length_in_tokens\": 500,\n",
      "        \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 500,\n",
      "        \"max_length_in_tokens\": 1000,\n",
      "        \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1000,\n",
      "        \"max_length_in_tokens\": 2000,\n",
      "        \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 2000,\n",
      "        \"max_length_in_tokens\": 3000,\n",
      "        \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 4000,\n",
      "        \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 305,\n",
      "        \"max_length_in_tokens\": 3997,\n",
      "        \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 100,\n",
      "        \"max_length_in_tokens\": 250,\n",
      "        \"payload_file\": \"payload_en_100-250.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 10,\n",
      "        \"max_length_in_tokens\": 20,\n",
      "        \"payload_file\": \"payload_en_10-20.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 50,\n",
      "        \"max_length_in_tokens\": 150,\n",
      "        \"payload_file\": \"payload_en_50-150.jsonl\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_50-150\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.m5.xlarge\": 0.23,\n",
      "    \"ml.g5.xlarge\": 1.006,\n",
      "    \"ml.g5.2xlarge\": 1.212,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688,\n",
      "    \"ml.p3.2xlarge\": 3.825\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"ContentType\": \"application/x-text\",\n",
      "    \"Accept\": \"application/json;verbose\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"distilbert-base-uncased-ml-p3-2xlarge\",\n",
      "      \"model_id\": \"huggingface-tc-distilbert-base-uncased\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"distilbert-base-uncased\",\n",
      "      \"ep_name\": \"distilbert-base-uncased\",\n",
      "      \"instance_type\": \"ml.p3.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"inference_script\": \"sagemaker_predictor.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_50-150.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"distilbert-base-uncased-ml-m5-xlarge\",\n",
      "      \"model_id\": \"huggingface-tc-distilbert-base-uncased\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"distilbert-base-uncased\",\n",
      "      \"ep_name\": \"distilbert-base-uncased\",\n",
      "      \"instance_type\": \"ml.m5.xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"inference_script\": \"sagemaker_predictor.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_50-150.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"report\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\",\n",
      "    \"txn_count_for_showing_cost\": 100000,\n",
      "    \"v_shift_w_single_instance\": 0.025,\n",
      "    \"v_shift_w_gt_one_instance\": 0.025\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## config.yml file contains information that is used across this benchmarking environment, \n",
    "## such as information about the aws account, prompts, payloads to be used for invocations\n",
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the file path for the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:52,733] p30320 {3750880026.py:12} INFO - Using the prompt template from S3 --> {text}\n",
      "[2024-03-20 01:23:52,734] p30320 {3750880026.py:21} INFO - empty prompt template = \"\"\n",
      "[2024-03-20 01:23:52,735] p30320 {3750880026.py:25} INFO - prompt template length=1 tokens\n"
     ]
    }
   ],
   "source": [
    "s3_file_path = \"/\".join([config['s3_read_data']['prompt_template_dir'],\n",
    "                         config['s3_read_data']['prompt_template_file']])\n",
    "\n",
    "## download the file from s3 else check locally and use that version\n",
    "prompt_template_from_s3: str = read_from_s3(config['s3_read_data']['read_bucket'], s3_file_path)\n",
    "\n",
    "if prompt_template_from_s3 is None:\n",
    "    prompt_template = globals.PROMPT_TEMPLATE\n",
    "    logger.info(f\"Using the default local prompt template --> {prompt_template}\")\n",
    "else:\n",
    "    prompt_template = prompt_template_from_s3\n",
    "    logger.info(f\"Using the prompt template from S3 --> {prompt_template}\")\n",
    "prompt_template = prompt_template.strip()\n",
    "\n",
    "# Calculate the number of tokens in the prompt template\n",
    "prompt_template_keys = config['datasets']['prompt_template_keys']\n",
    "args = {}\n",
    "for k in prompt_template_keys:\n",
    "    args[k] = \"\"\n",
    "empty_prompt_template = prompt_template.format(**args)\n",
    "logger.info(f\"empty prompt template = \\\"{empty_prompt_template}\\\"\")\n",
    "empty_prompt_len_in_tokens = count_tokens(empty_prompt_template)\n",
    "\n",
    "# Log the number of tokens\n",
    "logger.info(f\"prompt template length={empty_prompt_len_in_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:52,813] p30320 {490214810.py:7} INFO - s3 paths of the data set -> ['source_data/banking77.jsonl', 'source_data1/banking77.jsonl']\n",
      "[2024-03-20 01:23:52,814] p30320 {490214810.py:10} INFO - dataset files = ['source_data/banking77.jsonl', 'source_data1/banking77.jsonl']\n",
      "[2024-03-20 01:23:53,018] p30320 {490214810.py:20} INFO - dataset read from ['source_data/banking77.jsonl', 'source_data1/banking77.jsonl']\n",
      "has shape (20006, 2)\n"
     ]
    }
   ],
   "source": [
    "def list_files():\n",
    "    response = s3_client.list_objects_v2(Bucket=config['s3_read_data']['read_bucket'], Prefix=config['s3_read_data']['source_data_prefix'])\n",
    "    return [obj['Key'] for obj in response['Contents']]\n",
    "\n",
    "# List all files in the bucket and prefix\n",
    "s3_files = list_files()\n",
    "logger.info(f\"s3 paths of the data set -> {s3_files}\")\n",
    "\n",
    "# Log the files you're going to read\n",
    "logger.info(f\"dataset files = {s3_files}\")\n",
    "\n",
    "# Read and concatenate DataFrames\n",
    "jsonl_files = [file_key for file_key in s3_files if file_key.endswith('.jsonl')]\n",
    "\n",
    "# Read and concatenate only the .jsonl files\n",
    "df = pd.concat([pd.read_json(io.BytesIO(s3_client.get_object(Bucket=config['s3_read_data']['read_bucket'], Key=file_key)['Body'].read()), lines=True) \n",
    "                for file_key in jsonl_files])\n",
    "\n",
    "# Log the source of the dataset and its shape\n",
    "logger.info(f\"dataset read from {s3_files}\\nhas shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View a portion of the df to view inputs, contexts, and more information on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                     I am still waiting on my card?     11\n",
       "1  What can I do if my card still hasn't arrived ...     11\n",
       "2  I have been waiting over a week. Is the card s...     11\n",
       "3  Can I track my card while it is in the process...     11\n",
       "4  How do I know if I will get my card, or if it ...     11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display basic statistics on the existing dataset: including count, mean, std, min, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:53,060] p30320 {1912450148.py:1} INFO - distribution of the length field in the dataset is as follows ->\n",
      "              label\n",
      "count  20006.000000\n",
      "mean      37.651704\n",
      "std       22.390456\n",
      "min        0.000000\n",
      "25%       18.000000\n",
      "50%       37.000000\n",
      "75%       57.000000\n",
      "max       76.000000\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"distribution of the length field in the dataset is as follows ->\\n{df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset elements into prompts as payloads for inference purposes\n",
    "\n",
    "Now, we will focus on converting the existing data within our datasets, and extract the information to convert it into prompts to be able to send to our deployed model endpoints during the process of testing and benchmarking for results and various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 9.98 ms, total: 2.53 s\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['prompt'] = df.apply(lambda row: process_item(row, config['datasets']['prompt_template_keys'], prompt_template), axis=1)\n",
    "df['prompt_len'] = df.prompt.map(lambda x: x['prompt_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:56,681] p30320 {1138732359.py:11} INFO - all prompts dataframe of shape (20006, 4) saved to s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/all_prompts.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to a CSV format string\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "all_prompts_file = config['dir_paths']['all_prompts_file']\n",
    "\n",
    "# Write to S3 using the write_to_s3 function\n",
    "write_to_s3(csv_data, config['aws']['bucket'], DATA_DIR, config['dir_paths']['prompts_prefix'], all_prompts_file)\n",
    "\n",
    "# Log where the prompts are saved\n",
    "logger.info(f\"all prompts dataframe of shape {df.shape} saved to s3://{config['aws']['bucket']}/{DATA_DIR}/{os.path.join(config['dir_paths']['prompts_prefix'], all_prompts_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>11</td>\n",
       "      <td>{'text': 'I am still waiting on my card?', 'te...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'text': 'What can I do if my card still hasn'...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'text': 'I have been waiting over a week. Is ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'text': 'Can I track my card while it is in t...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'text': 'How do I know if I will get my card,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                     I am still waiting on my card?     11   \n",
       "1  What can I do if my card still hasn't arrived ...     11   \n",
       "2  I have been waiting over a week. Is the card s...     11   \n",
       "3  Can I track my card while it is in the process...     11   \n",
       "4  How do I know if I will get my card, or if it ...     11   \n",
       "\n",
       "                                              prompt  prompt_len  \n",
       "0  {'text': 'I am still waiting on my card?', 'te...           9  \n",
       "1  {'text': 'What can I do if my card still hasn'...          18  \n",
       "2  {'text': 'I have been waiting over a week. Is ...          15  \n",
       "3  {'text': 'Can I track my card while it is in t...          15  \n",
       "4  {'text': 'How do I know if I will get my card,...          18  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## View some of the prompts \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Prompts into Payloads for inference purposes\n",
    "------\n",
    "Now we will prepare data for model inference. It involves converting prompts, created and stored in a specific format, into payloads for inference. We will utilize the prompt file for our model and incorporate the prompt into a payload using that. \n",
    "\n",
    "These payloads are tailored to the needs of deployed model endpoints. The conversion considers prompt sizes and specific configurations to further make our benchmarking more detailed and comprehensive. \n",
    "\n",
    "The goal is to have a set of well-formatted and parameterized payload requests of various sizes ready to be sent to the model endpoints for inference, with the responses to be used for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct a single request payload based on row prompt data and configuration\n",
    "def construct_request_payload(row, config: Dict) -> Dict:\n",
    "    \n",
    "    # Deep copy inference parameters from the config.yml file - feel free to change this based on the model type you are using\n",
    "    parameters = copy.deepcopy(config['inference_parameters'])\n",
    "    truncate = parameters.get('truncate', None)\n",
    "    if truncate == TRUNCATE_POLICY.AT_PROMPT_TOKEN_LENGTH:\n",
    "        parameters['truncate'] = row['prompt_len']\n",
    "        \n",
    "    # Return the constructed payload\n",
    "    return dict(inputs=row['prompt']['prompt'], parameters=parameters)\n",
    "\n",
    "# Function to create a dataset payload files from the given dataset file we have\n",
    "def create_dataset_payload_file(df: pd.DataFrame, dataset_info: Dict, config: Dict) -> str:\n",
    "    \n",
    "    # First, log the dataset existing information\n",
    "    logger.info(f\"going to create a payload file as dataset_info={json.dumps(dataset_info, indent=2)}\")\n",
    "    \n",
    "    # Filter the DataFrame based on prompt length and language given below for constructing payloads of various sizes\n",
    "    df['prompt_len_in_range'] = df.prompt.map(lambda x: x['prompt_len'] >= dataset_info['min_length_in_tokens'] and \\\n",
    "                                                        x['prompt_len'] <= dataset_info['max_length_in_tokens'])\n",
    "    \n",
    "    # select prompts between pre-configured threshold lengths and are in the selected language\n",
    "    if 'language' in df.columns:\n",
    "        df_filtered = df[(df.language == dataset_info['language']) & (df.prompt_len_in_range)]\n",
    "    else:\n",
    "        df_filtered = df[df.prompt_len_in_range]\n",
    "        \n",
    "    logger.info(f\"after filtering for {json.dumps(dataset_info, indent=2)}, shape of dataframe is {df_filtered.shape}\")\n",
    "    if df_filtered.shape[0] == 0:\n",
    "        logger.error(f\"did not find any prompts in the dataframe that matched the filtering criteria, exiting\")\n",
    "        return None\n",
    "    # df_filtered.head()\n",
    "\n",
    "    # Here, we construct request payloads for each row in the filtered DataFrame\n",
    "    df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
    "    logger.info(f\"payload request entry looks like this -> {json.dumps(df_filtered['request'].iloc[0], indent=2)}\")\n",
    "    \n",
    "     # Convert the 'request' column of the filtered DataFrame to a JSON Lines string\n",
    "    json_lines_str = df_filtered['request'].to_json(orient='records', lines=True)\n",
    "    \n",
    "    \n",
    "    lang = dataset_info['language']\n",
    "    min_len = dataset_info['min_length_in_tokens']\n",
    "    max_len = dataset_info['max_length_in_tokens']\n",
    "    file_name = dataset_info['payload_file'].format(lang=lang, min=min_len, max=max_len)\n",
    "\n",
    "    prompts_path = os.path.join(DATA_DIR, config['dir_paths']['prompts_prefix'])\n",
    "\n",
    "    ## defining the s3_path these prompts will go to\n",
    "    s3_file_path = os.path.join(prompts_path, file_name)\n",
    "\n",
    "    # Write the JSON Lines string to S3\n",
    "    # get the bucket name, config vars from config file\n",
    "    write_to_s3(json_lines_str, config['aws']['bucket'], DATA_DIR, config['dir_paths']['prompts_prefix'], file_name)\n",
    "\n",
    "    logger.info(f\"dataset of different payload file structures saved to s3://{config['aws']['bucket']}/{s3_file_path}\")\n",
    "    return f\"s3://{config['aws']['bucket']}/{s3_file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-20 01:23:56,708] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 1,\n",
      "  \"max_length_in_tokens\": 500,\n",
      "  \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:56,719] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 1,\n",
      "  \"max_length_in_tokens\": 500,\n",
      "  \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "}, shape of dataframe is (20006, 5)\n",
      "[2024-03-20 01:23:56,882] p30320 {2925969590.py:37} INFO - payload request entry looks like this -> {\n",
      "  \"inputs\": \"I am still waiting on my card?\",\n",
      "  \"parameters\": {\n",
      "    \"ContentType\": \"application/x-text\",\n",
      "    \"Accept\": \"application/json;verbose\"\n",
      "  }\n",
      "}\n",
      "[2024-03-20 01:23:57,136] p30320 {2925969590.py:57} INFO - dataset of different payload file structures saved to s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_1-500.jsonl\n",
      "[2024-03-20 01:23:57,141] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 500,\n",
      "  \"max_length_in_tokens\": 1000,\n",
      "  \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,148] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 500,\n",
      "  \"max_length_in_tokens\": 1000,\n",
      "  \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "}, shape of dataframe is (0, 5)\n",
      "[2024-03-20 01:23:57,148] p30320 {2925969590.py:31} ERROR - did not find any prompts in the dataframe that matched the filtering criteria, exiting\n",
      "[2024-03-20 01:23:57,149] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 1000,\n",
      "  \"max_length_in_tokens\": 2000,\n",
      "  \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,156] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 1000,\n",
      "  \"max_length_in_tokens\": 2000,\n",
      "  \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "}, shape of dataframe is (0, 5)\n",
      "[2024-03-20 01:23:57,156] p30320 {2925969590.py:31} ERROR - did not find any prompts in the dataframe that matched the filtering criteria, exiting\n",
      "[2024-03-20 01:23:57,157] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 2000,\n",
      "  \"max_length_in_tokens\": 3000,\n",
      "  \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,163] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 2000,\n",
      "  \"max_length_in_tokens\": 3000,\n",
      "  \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "}, shape of dataframe is (0, 5)\n",
      "[2024-03-20 01:23:57,163] p30320 {2925969590.py:31} ERROR - did not find any prompts in the dataframe that matched the filtering criteria, exiting\n",
      "[2024-03-20 01:23:57,164] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 3000,\n",
      "  \"max_length_in_tokens\": 4000,\n",
      "  \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,170] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 3000,\n",
      "  \"max_length_in_tokens\": 4000,\n",
      "  \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "}, shape of dataframe is (0, 5)\n",
      "[2024-03-20 01:23:57,170] p30320 {2925969590.py:31} ERROR - did not find any prompts in the dataframe that matched the filtering criteria, exiting\n",
      "[2024-03-20 01:23:57,171] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 305,\n",
      "  \"max_length_in_tokens\": 3997,\n",
      "  \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,176] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 305,\n",
      "  \"max_length_in_tokens\": 3997,\n",
      "  \"payload_file\": \"payload_en_305-3997.jsonl\"\n",
      "}, shape of dataframe is (0, 5)\n",
      "[2024-03-20 01:23:57,176] p30320 {2925969590.py:31} ERROR - did not find any prompts in the dataframe that matched the filtering criteria, exiting\n",
      "[2024-03-20 01:23:57,177] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 100,\n",
      "  \"max_length_in_tokens\": 250,\n",
      "  \"payload_file\": \"payload_en_100-250.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,183] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 100,\n",
      "  \"max_length_in_tokens\": 250,\n",
      "  \"payload_file\": \"payload_en_100-250.jsonl\"\n",
      "}, shape of dataframe is (2, 5)\n",
      "/tmp/ipykernel_30320/2925969590.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
      "[2024-03-20 01:23:57,185] p30320 {2925969590.py:37} INFO - payload request entry looks like this -> {\n",
      "  \"inputs\": \"Hearing back from us regarding your important verification results may take 10 minutes to one hour time.  If verification results do fail, double-check to make sure all of your images are clear --  make sure your photos have no glare or blurring. Note: These photos need to be readable as well.  You also need to be 18 years of age or older.  You must be a resident of Switzerland or the European Economic Area to open a new account.\",\n",
      "  \"parameters\": {\n",
      "    \"ContentType\": \"application/x-text\",\n",
      "    \"Accept\": \"application/json;verbose\"\n",
      "  }\n",
      "}\n",
      "[2024-03-20 01:23:57,257] p30320 {2925969590.py:57} INFO - dataset of different payload file structures saved to s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_100-250.jsonl\n",
      "[2024-03-20 01:23:57,258] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 10,\n",
      "  \"max_length_in_tokens\": 20,\n",
      "  \"payload_file\": \"payload_en_10-20.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,267] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 10,\n",
      "  \"max_length_in_tokens\": 20,\n",
      "  \"payload_file\": \"payload_en_10-20.jsonl\"\n",
      "}, shape of dataframe is (13126, 5)\n",
      "/tmp/ipykernel_30320/2925969590.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
      "[2024-03-20 01:23:57,376] p30320 {2925969590.py:37} INFO - payload request entry looks like this -> {\n",
      "  \"inputs\": \"What can I do if my card still hasn't arrived after 2 weeks?\",\n",
      "  \"parameters\": {\n",
      "    \"ContentType\": \"application/x-text\",\n",
      "    \"Accept\": \"application/json;verbose\"\n",
      "  }\n",
      "}\n",
      "[2024-03-20 01:23:57,683] p30320 {2925969590.py:57} INFO - dataset of different payload file structures saved to s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_10-20.jsonl\n",
      "[2024-03-20 01:23:57,688] p30320 {2925969590.py:17} INFO - going to create a payload file as dataset_info={\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 50,\n",
      "  \"max_length_in_tokens\": 150,\n",
      "  \"payload_file\": \"payload_en_50-150.jsonl\"\n",
      "}\n",
      "[2024-03-20 01:23:57,701] p30320 {2925969590.py:29} INFO - after filtering for {\n",
      "  \"language\": \"en\",\n",
      "  \"min_length_in_tokens\": 50,\n",
      "  \"max_length_in_tokens\": 150,\n",
      "  \"payload_file\": \"payload_en_50-150.jsonl\"\n",
      "}, shape of dataframe is (308, 5)\n",
      "/tmp/ipykernel_30320/2925969590.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
      "[2024-03-20 01:23:57,709] p30320 {2925969590.py:37} INFO - payload request entry looks like this -> {\n",
      "  \"inputs\": \"I'm pretty frustrated about a pound charge that keeps  showing up in my account statement. It's listed in the app as \\\"pending\\\", it never changes, and now I'm thinking someone's hacked my account!\",\n",
      "  \"parameters\": {\n",
      "    \"ContentType\": \"application/x-text\",\n",
      "    \"Accept\": \"application/json;verbose\"\n",
      "  }\n",
      "}\n",
      "[2024-03-20 01:23:57,813] p30320 {2925969590.py:57} INFO - dataset of different payload file structures saved to s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_50-150.jsonl\n"
     ]
    }
   ],
   "source": [
    "items = ((df, d, config) for d in config['datasets']['filters'])\n",
    "\n",
    "# This results in the creation of payload files for each dataset\n",
    "paths: List = list(itertools.starmap(create_dataset_payload_file, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_1-500.jsonl\n",
      "s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_100-250.jsonl\n",
      "s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_10-20.jsonl\n",
      "s3://sagemaker-fmbench-write-015469603702/distilbert-base-uncased-v1-SageMaker/data/prompts/payload_en_50-150.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([p for p in paths if p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_my_python311",
   "language": "python",
   "name": "conda_my_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
